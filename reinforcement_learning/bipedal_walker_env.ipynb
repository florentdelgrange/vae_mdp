{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import os\n",
    "import PIL.Image\n",
    "from tensorflow.python.keras.utils.generic_utils import Progbar\n",
    "from tf_agents.environments import suite_gym, parallel_py_environment\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.trajectories.trajectory import Trajectory\n",
    "from tf_agents.networks import actor_distribution_network\n",
    "from tf_agents.drivers import dynamic_episode_driver, dynamic_step_driver\n",
    "from tf_agents.policies import random_tf_policy\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from reinforcement_learning.sac_training import NumberOfSafetyViolations\n",
    "from tf_agents.trajectories import time_step as ts\n",
    "from reinforcement_learning import sac_training\n",
    "from reinforcement_learning import labeling_functions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Bipedal walker v2 environment\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "py_env = suite_gym.load('BipedalWalker-v2')\n",
    "py_env.render(mode='human')\n",
    "py_env.reset()\n",
    "tf_env = tf_py_environment.TFPyEnvironment(py_env)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bad_state_detection = lambda trajectory: print(\"bad state!: {}\".format(trajectory.observation[..., 0]))\\\n",
    "    if tf.math.abs(trajectory.observation[...,0]) > np.pi / 3 else None\n",
    "\n",
    "tf_env = tf_py_environment.TFPyEnvironment(py_env)\n",
    "policy = random_tf_policy.RandomTFPolicy(time_step_spec=tf_env.time_step_spec(), action_spec=tf_env.action_spec())\n",
    "dynamic_episode_driver.DynamicEpisodeDriver(tf_env, policy, num_episodes=15,\n",
    "                                            observers=[bad_state_detection, lambda _: py_env.render(mode='human')]).run()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Perform a few steps in this environment\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def bad_state_detection(trajectory):\n",
    "    # if trajectory.reward[..., 0] <= -100:\n",
    "    if trajectory.observation[..., 0] < -1. or trajectory.observation[..., 0] > 1:\n",
    "        py_env.render(mode='human')\n",
    "        print(trajectory.observation)\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "walk = True\n",
    "while walk:\n",
    "    action = policy.action(time_step=tf_env.current_time_step())\n",
    "    time_step = tf_env.step(action)\n",
    "    walk = not bad_state_detection(time_step)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%  bad states\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from reinforcement_learning import labeling_functions\n",
    "\n",
    "labeling_function = labeling_functions['BipedalWalker-v2']\n",
    "safety_violations = NumberOfSafetyViolations(labeling_function)\n",
    "progressbar = Progbar(target=None, interval=0.5, stateful_metrics=['violation'])\n",
    "\n",
    "tf_env = tf_py_environment.TFPyEnvironment(py_env)\n",
    "policy = random_tf_policy.RandomTFPolicy(\n",
    "    time_step_spec=tf_env.time_step_spec(),\n",
    "    action_spec=tf_env.action_spec())\n",
    "dynamic_episode_driver.DynamicEpisodeDriver(\n",
    "    tf_env,\n",
    "    policy,\n",
    "    num_episodes=15,\n",
    "    observers=[safety_violations,\n",
    "               lambda _: progressbar.add(\n",
    "                   1, [('violation', safety_violations.average())]),\n",
    "               lambda _: py_env.render(mode='human')]\n",
    ").run()\n",
    "\n",
    "safety_violations._num_episodes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% safety violation function\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Parallel environments.\n",
    "num_parallel_environments = 16\n",
    "tf_env = tf_py_environment.TFPyEnvironment(\n",
    "    parallel_py_environment.ParallelPyEnvironment(\n",
    "    [lambda : suite_gym.load('BipedalWalker-v2')] * num_parallel_environments))\n",
    "tf_env.reset()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% parallel environments\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "labeling_function = labeling_functions['BipedalWalker-v2']\n",
    "\n",
    "policy = random_tf_policy.RandomTFPolicy(\n",
    "    time_step_spec=tf_env.time_step_spec(),\n",
    "    action_spec=tf_env.action_spec())\n",
    "\n",
    "safety_violations = NumberOfSafetyViolations(labeling_function)\n",
    "\n",
    "dynamic_step_driver.DynamicStepDriver(\n",
    "    tf_env,\n",
    "    policy,\n",
    "    num_steps=5000,\n",
    "    observers=[safety_violations]\n",
    ").run()\n",
    "\n",
    "print('Safety violations')\n",
    "print('episodes', safety_violations._num_episodes)\n",
    "print('result=', safety_violations.result())\n",
    "print('average=', safety_violations.average())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%  labeling function/unsafe states detection for parallel environments\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import importlib\n",
    "from tf_agents.environments import suite_gym\n",
    "from reinforcement_learning import labeling_functions\n",
    "from reinforcement_learning import sac_training\n",
    "\n",
    "importlib.reload(sac_training)\n",
    "\n",
    "learner = sac_training.SACLearner(\n",
    "    env_name='BipedalWalker-v2',\n",
    "    env_suite=suite_gym,\n",
    "    num_iterations=int(1e6),\n",
    "    num_parallel_environments=8,\n",
    "    labeling_function=labeling_functions['BipedalWalker-v2']\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% SAC learning\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "learner.train_and_eval()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Before running this cell, load the single py environment\n",
    "tf_env = tf_py_environment.TFPyEnvironment(py_env)\n",
    "stochastic_policy_dir = \"../saves/BipedalWalker-v2/policy\"\n",
    "policy = tf.compat.v2.saved_model.load(stochastic_policy_dir)\n",
    "dynamic_episode_driver.DynamicEpisodeDriver(\n",
    "    tf_env,\n",
    "    policy,\n",
    "    num_episodes=15,\n",
    "    observers=[lambda _: py_env.render(mode='human')]\n",
    ").run()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Load and try the SAC policy\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from reinforcement_learning import sac_training\n",
    "\n",
    "learner = sac_training.SACLearner(\n",
    "    env_name='BipedalWalker-v2',\n",
    "    env_suite=suite_gym,\n",
    "    labeling_function=labeling_functions['BipedalWalker-v2'],\n",
    "    save_directory_location='..'\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Create a permissive variance policy\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "variance_multiplier = 3."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "learner.save_permissive_variance_policy(variance_multiplier=variance_multiplier)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Before running this cell, load the single py environment\n",
    "stochastic_policy_dir = os.path.join(\n",
    "    learner.save_directory_location,\n",
    "    'policy',\n",
    "    \"permissive_variance_policy-multiplier={}\".format(\n",
    "        variance_multiplier)\n",
    ")\n",
    "policy = tf.compat.v2.saved_model.load(stochastic_policy_dir)\n",
    "safety_violations = NumberOfSafetyViolations(\n",
    "    labeling_function=labeling_functions['BipedalWalker-v2'])\n",
    "\n",
    "dynamic_episode_driver.DynamicEpisodeDriver(\n",
    "    tf_env,\n",
    "    policy,\n",
    "    num_episodes=1000,\n",
    "    observers=[\n",
    "        #  lambda _: py_env.render(mode='human'),\n",
    "        safety_violations\n",
    "    ]\n",
    ").run()\n",
    "\n",
    "print(\"avg number of safety violations per episode\", safety_violations.average())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Load and try the permissive variance policy\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAE MDP loaded\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-6-185a27eecdf8>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     30\u001B[0m     \u001B[0mnext_state\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtf_env\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcurrent_time_step\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mobservation\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     31\u001B[0m     next_label = tf.expand_dims(\n\u001B[0;32m---> 32\u001B[0;31m         \u001B[0mtf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcast\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlabeling_functions\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'BipedalWalker-v2'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnext_state\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfloat32\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     33\u001B[0m         \u001B[0maxis\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     34\u001B[0m     )\n",
      "\u001B[0;32m~/PycharmProjects/vae_mdp/reinforcement_learning/__init__.py\u001B[0m in \u001B[0;36m<lambda>\u001B[0;34m(observation)\u001B[0m\n\u001B[1;32m      7\u001B[0m     \u001B[0;31m# np.count_nonzero(np.abs(observation[:, :, 8: 42][0::2]) > 0.99) > 0  # has stuck joints\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      8\u001B[0m     \u001B[0;34m'BipedalWalker-v2'\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 9\u001B[0;31m         \u001B[0;32mlambda\u001B[0m \u001B[0mobservation\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mtf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmath\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mabs\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mobservation\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m...\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m>\u001B[0m \u001B[0mmath\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpi\u001B[0m \u001B[0;34m/\u001B[0m \u001B[0;36m3.\u001B[0m  \u001B[0;31m# hull angle too high/low\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     10\u001B[0m }\n",
      "\u001B[0;32m~/anaconda3/envs/vae-mdp/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    178\u001B[0m     \u001B[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    179\u001B[0m     \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 180\u001B[0;31m       \u001B[0;32mreturn\u001B[0m \u001B[0mtarget\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    181\u001B[0m     \u001B[0;32mexcept\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mTypeError\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mValueError\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    182\u001B[0m       \u001B[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/vae-mdp/lib/python3.8/site-packages/tensorflow/python/ops/math_ops.py\u001B[0m in \u001B[0;36mabs\u001B[0;34m(x, name)\u001B[0m\n\u001B[1;32m    266\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdtype\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mis_complex\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    267\u001B[0m       \u001B[0;32mreturn\u001B[0m \u001B[0mgen_math_ops\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcomplex_abs\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mTout\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdtype\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mreal_dtype\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 268\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0mgen_math_ops\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_abs\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    269\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    270\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/vae-mdp/lib/python3.8/site-packages/tensorflow/python/ops/gen_math_ops.py\u001B[0m in \u001B[0;36m_abs\u001B[0;34m(x, name)\u001B[0m\n\u001B[1;32m     36\u001B[0m   \"\"\"\n\u001B[1;32m     37\u001B[0m   \u001B[0m_ctx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_context\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_context\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0m_context\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcontext\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 38\u001B[0;31m   \u001B[0mtld\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_ctx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_thread_local_data\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     39\u001B[0m   \u001B[0;32mif\u001B[0m \u001B[0mtld\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mis_eager\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     40\u001B[0m     \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "import variational_action_discretizer\n",
    "from variational_action_discretizer import VariationalActionDiscretizer\n",
    "\n",
    "vae_mdp = variational_action_discretizer.load(\n",
    "    '../saves/BipedalWalker-v2/models/vae_LS14_MC5_CER10.0_KLA0.0_TD1.00-0.90_1e-06-2e-06_step410000_eval_elbo52.650/permissive_variance_policy-multiplier=3.0/action_discretizer/LA6_MC16_CER1.0-decay=0.001_KLA0.0-growth=5e-06_TD0.20-0.13_1e-06-2e-06_decoder_divergence0.1_params=one_output_per_action/step200000/eval_elbo-1.073'\n",
    ")\n",
    "print(\"VAE MDP loaded\")\n",
    "tf_env.reset()\n",
    "policy = vae_mdp.generate_random_policy(tf_env)\n",
    "\n",
    "safety_violations = NumberOfSafetyViolations(\n",
    "    labeling_function=labeling_functions['BipedalWalker-v2'])\n",
    "\n",
    "initial_state = tf.zeros(\n",
    "    shape=tf_env.time_step_spec().observation.shape, dtype=tf.float32)\n",
    "initial_action = tf.zeros(shape=tf_env.action_spec().shape, dtype=tf.float32)\n",
    "initial_reward = tf.zeros(\n",
    "    shape=(1, ), dtype=tf.float32\n",
    ")\n",
    "\n",
    "state, action, reward = [\n",
    "    tf.stack([initial_state for _ in range(tf_env.batch_size)]),\n",
    "    tf.stack([initial_action for _ in range(tf_env.batch_size)]),\n",
    "    tf.stack([initial_reward for _ in range(tf_env.batch_size)]),\n",
    "]\n",
    "\n",
    "num_steps = 5000\n",
    "for _ in range(num_steps):\n",
    "    py_env.render(mode='human')\n",
    "    next_state = tf_env.current_time_step().observation\n",
    "    next_label = tf.expand_dims(\n",
    "        tf.cast(labeling_functions['BipedalWalker-v2'](next_state), tf.float32),\n",
    "        axis=-1\n",
    "    )\n",
    "    z = vae_mdp.binary_encode(state, action, reward, next_state, next_label).sample()\n",
    "    state = next_state\n",
    "    action = policy.action(\n",
    "        time_step=ts.TimeStep(\n",
    "            step_type=tf_env.current_time_step().step_type,\n",
    "            reward=tf_env.current_time_step().reward,\n",
    "            discount=tf_env.current_time_step().discount,\n",
    "            observation=z,\n",
    "        )\n",
    "    ).action\n",
    "    time_step = tf_env.step(action)\n",
    "    reward = tf.expand_dims(time_step.reward, axis=-1)  # scalar reward\n",
    "    safety_violations(time_step)\n",
    "\n",
    "print(\"avg number of safety violations per episode\", safety_violations.average())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% generate a random policy over discrete action learned through the VAE-MDP\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "(<tf.Tensor: shape=(1, 24), dtype=float32, numpy=\n array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)>,\n <tf.Tensor: shape=(1, 4), dtype=float32, numpy=array([[0., 0., 0., 0.]], dtype=float32)>,\n <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n <tf.Tensor: shape=(1, 24), dtype=float32, numpy=\n array([[ 2.7470193e-03,  3.3419803e-06, -4.3579831e-04, -1.6000021e-02,\n          9.2256993e-02,  1.0116901e-03,  8.6004359e-01,  5.9883302e-04,\n          1.0000000e+00,  3.2633666e-02,  1.0116381e-03,  8.5365462e-01,\n         -6.8835629e-04,  1.0000000e+00,  4.4081375e-01,  4.4581985e-01,\n          4.6142250e-01,  4.8954991e-01,  5.3410250e-01,  6.0246068e-01,\n          7.0914847e-01,  8.8593131e-01,  1.0000000e+00,  1.0000000e+00]],\n       dtype=float32)>,\n <tf.Tensor: shape=(1,), dtype=bool, numpy=array([False])>)"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state, action, reward, next_state, next_label\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}