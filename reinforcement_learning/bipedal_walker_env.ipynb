{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Tuple\n",
    "from tensorflow.python.keras.utils.generic_utils import Progbar\n",
    "from tf_agents.environments import suite_gym, parallel_py_environment\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer, episodic_replay_buffer\n",
    "from tf_agents.trajectories.trajectory import Trajectory\n",
    "from tf_agents.networks import actor_distribution_network\n",
    "from tf_agents.drivers import dynamic_episode_driver, dynamic_step_driver\n",
    "from tf_agents.policies import random_tf_policy\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tf_agents.utils import common\n",
    "from reinforcement_learning.sac_training import NumberOfSafetyViolations\n",
    "from tf_agents.trajectories import time_step as ts, policy_step, trajectory\n",
    "from reinforcement_learning import sac_training\n",
    "from reinforcement_learning import labeling_functions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Bipedal walker v2 environment\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "TimeStep(step_type=TensorSpec(shape=(), dtype=tf.int32, name='step_type'), reward=TensorSpec(shape=(), dtype=tf.float32, name='reward'), discount=BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)), observation=BoundedTensorSpec(shape=(24,), dtype=tf.float32, name='observation', minimum=array(-3.4028235e+38, dtype=float32), maximum=array(3.4028235e+38, dtype=float32)))"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "py_env = suite_gym.load('BipedalWalker-v2')\n",
    "py_env.render(mode='human')\n",
    "py_env.reset()\n",
    "tf_env = tf_py_environment.TFPyEnvironment(py_env)\n",
    "tf_env.time_step_spec()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bad_state_detection = lambda trajectory: print(\"bad state!: {}\".format(trajectory.observation[..., 0]))\\\n",
    "    if tf.math.abs(trajectory.observation[...,0]) > np.pi / 3 else None\n",
    "\n",
    "tf_env = tf_py_environment.TFPyEnvironment(py_env)\n",
    "policy = random_tf_policy.RandomTFPolicy(time_step_spec=tf_env.time_step_spec(), action_spec=tf_env.action_spec())\n",
    "dynamic_episode_driver.DynamicEpisodeDriver(tf_env, policy, num_episodes=15,\n",
    "                                            observers=[bad_state_detection, lambda _: py_env.render(mode='human')]).run()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Perform a few steps in this environment\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def bad_state_detection(trajectory):\n",
    "    # if trajectory.reward[..., 0] <= -100:\n",
    "    if trajectory.observation[..., 0] < -1. or trajectory.observation[..., 0] > 1:\n",
    "        py_env.render(mode='human')\n",
    "        print(trajectory.observation)\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "walk = True\n",
    "while walk:\n",
    "    action = policy.action(time_step=tf_env.current_time_step())\n",
    "    time_step = tf_env.step(action)\n",
    "    walk = not bad_state_detection(time_step)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%  bad states\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from reinforcement_learning import labeling_functions\n",
    "\n",
    "labeling_function = labeling_functions['BipedalWalker-v2']\n",
    "safety_violations = NumberOfSafetyViolations(labeling_function)\n",
    "progressbar = Progbar(target=None, interval=0.5, stateful_metrics=['violation'])\n",
    "\n",
    "tf_env = tf_py_environment.TFPyEnvironment(py_env)\n",
    "policy = random_tf_policy.RandomTFPolicy(\n",
    "    time_step_spec=tf_env.time_step_spec(),\n",
    "    action_spec=tf_env.action_spec())\n",
    "dynamic_episode_driver.DynamicEpisodeDriver(\n",
    "    tf_env,\n",
    "    policy,\n",
    "    num_episodes=15,\n",
    "    observers=[safety_violations,\n",
    "               lambda _: progressbar.add(\n",
    "                   1, [('violation', safety_violations.average())]),\n",
    "               lambda _: py_env.render(mode='human')]\n",
    ").run()\n",
    "\n",
    "safety_violations._num_episodes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% safety violation function\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "TimeStep(step_type=TensorSpec(shape=(), dtype=tf.int32, name='step_type'), reward=TensorSpec(shape=(), dtype=tf.float32, name='reward'), discount=BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)), observation=BoundedTensorSpec(shape=(24,), dtype=tf.float32, name='observation', minimum=array(-3.4028235e+38, dtype=float32), maximum=array(3.4028235e+38, dtype=float32)))"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parallel environments.\n",
    "num_parallel_environments = 4\n",
    "tf_env = tf_py_environment.TFPyEnvironment(\n",
    "    parallel_py_environment.ParallelPyEnvironment(\n",
    "    [lambda : suite_gym.load('BipedalWalker-v2')] * num_parallel_environments))\n",
    "tf_env.reset()\n",
    "tf_env.time_step_spec()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% parallel environments\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "labeling_function = labeling_functions['BipedalWalker-v2']\n",
    "\n",
    "policy = random_tf_policy.RandomTFPolicy(\n",
    "    time_step_spec=tf_env.time_step_spec(),\n",
    "    action_spec=tf_env.action_spec())\n",
    "\n",
    "safety_violations = NumberOfSafetyViolations(labeling_function)\n",
    "\n",
    "dynamic_step_driver.DynamicStepDriver(\n",
    "    tf_env,\n",
    "    policy,\n",
    "    num_steps=5000,\n",
    "    observers=[safety_violations]\n",
    ").run()\n",
    "\n",
    "print('Safety violations')\n",
    "print('episodes', safety_violations._num_episodes)\n",
    "print('result=', safety_violations.result())\n",
    "print('average=', safety_violations.average())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%  labeling function/unsafe states detection for parallel environments\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import importlib\n",
    "from tf_agents.environments import suite_gym\n",
    "from reinforcement_learning import labeling_functions\n",
    "from reinforcement_learning import sac_training\n",
    "\n",
    "importlib.reload(sac_training)\n",
    "\n",
    "learner = sac_training.SACLearner(\n",
    "    env_name='BipedalWalker-v2',\n",
    "    env_suite=suite_gym,\n",
    "    num_iterations=int(1e6),\n",
    "    num_parallel_environments=8,\n",
    "    labeling_function=labeling_functions['BipedalWalker-v2']\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% SAC learning\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "learner.train_and_eval()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Before running this cell, load the single py environment\n",
    "tf_env = tf_py_environment.TFPyEnvironment(py_env)\n",
    "stochastic_policy_dir = \"../saves/BipedalWalker-v2/policy\"\n",
    "policy = tf.compat.v2.saved_model.load(stochastic_policy_dir)\n",
    "dynamic_episode_driver.DynamicEpisodeDriver(\n",
    "    tf_env,\n",
    "    policy,\n",
    "    num_episodes=15,\n",
    "    observers=[lambda _: py_env.render(mode='human')]\n",
    ").run()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Load and try the SAC policy\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from reinforcement_learning import sac_training\n",
    "\n",
    "learner = sac_training.SACLearner(\n",
    "    env_name='BipedalWalker-v2',\n",
    "    env_suite=suite_gym,\n",
    "    labeling_function=labeling_functions['BipedalWalker-v2'],\n",
    "    save_directory_location='..'\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Create a permissive variance policy\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "variance_multiplier = 3."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "learner.save_permissive_variance_policy(variance_multiplier=variance_multiplier)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Before running this cell, load the single py environment\n",
    "stochastic_policy_dir = os.path.join(\n",
    "    learner.save_directory_location,\n",
    "    'policy',\n",
    "    \"permissive_variance_policy-multiplier={}\".format(\n",
    "        variance_multiplier)\n",
    ")\n",
    "policy = tf.compat.v2.saved_model.load(stochastic_policy_dir)\n",
    "safety_violations = NumberOfSafetyViolations(\n",
    "    labeling_function=labeling_functions['BipedalWalker-v2'])\n",
    "\n",
    "dynamic_episode_driver.DynamicEpisodeDriver(\n",
    "    tf_env,\n",
    "    policy,\n",
    "    num_episodes=1000,\n",
    "    observers=[\n",
    "        #  lambda _: py_env.render(mode='human'),\n",
    "        safety_violations\n",
    "    ]\n",
    ").run()\n",
    "\n",
    "print(\"avg number of safety violations per episode\", safety_violations.average())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Load and try the permissive variance policy\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAE MDP loaded\n",
      "PolicyStep(action=[2], state=(), info=())\n",
      "avg number of safety violations per episode tf.Tensor(6.225806, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import variational_action_discretizer\n",
    "\n",
    "vae_mdp = variational_action_discretizer.load(\n",
    "    # \"/home/florentdelgrange/workspace/hpc_hydra/policy/Bipedal-walker/vae_LS15_MC16_CER10.0-decay=0.0015_KLA0.0-growth=5e-06_TD1.00-0.95_1e-06-2e-06_step400000_eval_elbo53.687/step3000000/eval_elbo0.227\"\n",
    "    \"../saves/BipedalWalker-v2/models/vae_LS13_MC3_CER10.0-decay=0.0015_KLA0.0-growth=5e-06_TD1.00-0.90_1e-06-2e\"\n",
    "    \"-06_params=relaxed_state_encoding_step320000_eval_elbo55.821/step320000/eval_elbo55.821/policy\"\n",
    "    \"/action_discretizer/LA5_MC1_CER1.0-decay=0.001_KLA0.0-growth=5e-06_TD0.25-0.17_1e-06-2e-06_params\"\n",
    "    \"=one_output_per_action-relaxed_state_encoding/step200000/eval_elbo-0.866\"\n",
    ")\n",
    "print(\"VAE MDP loaded\")\n",
    "\n",
    "discrete_tf_env = vae_mdp.wrap_tf_environment(\n",
    "    tf_env=tf_env,\n",
    "    labeling_function=labeling_functions['BipedalWalker-v2']\n",
    ")\n",
    "z = discrete_tf_env.reset()\n",
    "\n",
    "safety_violations = NumberOfSafetyViolations(\n",
    "    labeling_function=labeling_functions['BipedalWalker-v2'])\n",
    "policy = random_tf_policy.RandomTFPolicy(\n",
    "    time_step_spec=discrete_tf_env.time_step_spec(),\n",
    "    action_spec=discrete_tf_env.action_spec())\n",
    "\n",
    "tf.print(policy.action(z))\n",
    "\n",
    "dynamic_episode_driver.DynamicEpisodeDriver(\n",
    "    discrete_tf_env,\n",
    "    policy,\n",
    "    num_episodes=30,\n",
    "    observers=[\n",
    "        lambda _: py_env.render(mode='human'),\n",
    "        lambda _: safety_violations(tf_env.current_time_step())\n",
    "    ]\n",
    ").run()\n",
    "\n",
    "print(\"avg number of safety violations per episode\", safety_violations.average())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% generate a random policy over discrete action learned through the VAE-MDP\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAE MDP loaded\n",
      "avg number of safety violations per episode tf.Tensor(33.580647, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import variational_action_discretizer\n",
    "\n",
    "vae_mdp = variational_action_discretizer.load(\n",
    "    # \"/home/florentdelgrange/workspace/hpc_hydra/policy/Bipedal-walker/vae_LS15_MC16_CER10.0-decay=0.0015_KLA0.0-growth=5e-06_TD1.00-0.95_1e-06-2e-06_step400000_eval_elbo53.687/step3000000/eval_elbo0.227\"\n",
    "    \"../saves/BipedalWalker-v2/models/vae_LS13_MC3_CER10.0-decay=0.0015_KLA0.0-growth=5e-06_TD1.00-0.90_1e-06-2e\"\n",
    "    \"-06_params=relaxed_state_encoding_step320000_eval_elbo55.821/step320000/eval_elbo55.821/policy\"\n",
    "    \"/action_discretizer/LA5_MC1_CER1.0-decay=0.001_KLA0.0-growth=5e-06_TD0.25-0.17_1e-06-2e-06_params\"\n",
    "    \"=one_output_per_action-relaxed_state_encoding/step200000/eval_elbo-0.866\"\n",
    ")\n",
    "print(\"VAE MDP loaded\")\n",
    "\n",
    "discrete_tf_env = vae_mdp.wrap_tf_environment(\n",
    "    tf_env=tf_env,\n",
    "    labeling_function=labeling_functions['BipedalWalker-v2']\n",
    ")\n",
    "discrete_tf_env.reset()\n",
    "\n",
    "safety_violations = NumberOfSafetyViolations(\n",
    "    labeling_function=labeling_functions['BipedalWalker-v2'])\n",
    "policy = vae_mdp.get_simplified_policy()\n",
    "\n",
    "dynamic_episode_driver.DynamicEpisodeDriver(\n",
    "    discrete_tf_env,\n",
    "    policy,\n",
    "    num_episodes=30,\n",
    "    observers=[\n",
    "        lambda _: py_env.render(mode='human'),\n",
    "        lambda _: safety_violations(tf_env.current_time_step())\n",
    "    ]\n",
    ").run()\n",
    "\n",
    "print(\"avg number of safety violations per episode\", safety_violations.average())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Execute the simplified policy learned through the VAE latent prior\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}