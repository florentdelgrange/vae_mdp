{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%% LunarLander-v2\n"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "path = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "sys.path.insert(0, path + '/../..')\n",
    "\n",
    "import tf_agents.policies\n",
    "import tf_agents.specs\n",
    "from tf_agents.environments import suite_gym, parallel_py_environment\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.policies import policy_saver\n",
    "from tf_agents.policies.tf_py_policy import TFPyPolicy\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer, episodic_replay_buffer\n",
    "from tf_agents.drivers import dynamic_episode_driver, dynamic_step_driver\n",
    "import tensorflow as tf\n",
    "from tf_agents.trajectories.policy_step import PolicyStep\n",
    "from policies import SavedTFPolicy\n",
    "tf.config.set_visible_devices([], 'GPU')  #  allows testing during training\n",
    "from tf_agents.trajectories import time_step as ts, policy_step, trajectory\n",
    "from reinforcement_learning import labeling_functions\n",
    "labeling_function = labeling_functions['LunarLander-v2']\n",
    "from util.io.dataset_generator import map_rl_trajectory_to_vae_input\n",
    "from util.io.dataset_generator import ErgodicMDPTransitionGenerator\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions\n",
    "\n",
    "import reinforcement_learning.environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimeStep(step_type=TensorSpec(shape=(), dtype=tf.int32, name='step_type'), reward=TensorSpec(shape=(), dtype=tf.float32, name='reward'), discount=BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)), observation=BoundedTensorSpec(shape=(8,), dtype=tf.float32, name='observation', minimum=array(-3.4028235e+38, dtype=float32), maximum=array(3.4028235e+38, dtype=float32)))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "py_env = suite_gym.load('LunarLander-v2')\n",
    "py_env.reset()\n",
    "tf_env = tf_py_environment.TFPyEnvironment(py_env)\n",
    "tf_env.time_step_spec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def display_labeling(trajectory):\n",
    "    label = labeling_functions['LunarLanderContinuous-v2'](trajectory.observation)\n",
    "    if tf.reduce_any(label[..., 2]) and tf.reduce_any(label[..., 6]):\n",
    "        print('close to the lunar pad with high speed')\n",
    "    if not tf.reduce_any(label[..., 7]):\n",
    "        print('unsafe lander angle')\n",
    "    if tf.reduce_any(label[..., 2]) and not tf.reduce_any(label[..., 8]):\n",
    "        print('close to the lunar pad with unsafe lander angle')\n",
    "    if tf.reduce_any(label[..., 1]):\n",
    "        print('lander too close to the edge of the frame')\n",
    "\n",
    "reward_metric = tf_metrics.AverageReturnMetric()\n",
    "sac_policy_dir = '../saves/LunarLander-v2/dqn_policy'\n",
    "saved_policy = SavedTFPolicy(sac_policy_dir)\n",
    "dynamic_episode_driver.DynamicEpisodeDriver(tf_env, saved_policy, num_episodes=5,\n",
    "                                            observers=[\n",
    "                                                display_labeling,\n",
    "                                                lambda _: py_env.render(mode='human'),\n",
    "                                                reward_metric\n",
    "                                            ]).run()\n",
    "reward_metric.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import variational_mdp\n",
    "\n",
    "vae_mdp = variational_mdp.load(\n",
    "    # \"../../saves/Pendulum-v0/models/vae_LS12_MC1_ER20.0-decay=7.5e-05-min=-10_KLA0.0-growth=1e-06_TD0.70-0.50_1e-06-2e-06/policy/action_discretizer/LA3_MC1_ER20.0-decay=7.5e-05-min=-10_KLA0.0-growth=1e-06_TD0.50-0.33_1e-06-2e-06_params=full_vae_optimization/step140000/eval_elbo-3.784\"\n",
    "    # \"../../saves/Pendulum-v0/models/vae_LS13_MC1_ER20.0-decay=2e-05-min=-10_KLA0.0-growth=1e-06_TD0.70-0.50_1e-06-2e-06/policy/action_discretizer/LA5_MC1_ER20.0-decay=2e-05-min=-10_KLA0.0-growth=1e-06_TD0.25-0.17_1e-06-2e-06_params=full_vae_optimization/step270000/eval_elbo-1.704\"\n",
    "    # \"../../saves/Pendulum-v0/models/vae_LS13_MC1_ER20.0-decay=7.5e-05-min=-10_KLA0.0-growth=1e-06_TD0.70-0.50_1e-06-2e-06/permissive_variance_policy-multiplier=10.0/action_discretizer/LA5_MC1_ER20.0-decay=7.5e-05-min=-10_KLA0.0-growth=1e-06_TD0.25-0.17_1e-06-2e-06_params=full_vae_optimization/step70000/eval_elbo-7.265\"\n",
    "    # \"../../saves/Pendulum-v0/models/vae_LS12_MC3_CER100.0-decay=1e-05_KLA1e-06-growth=1e-07_TD0.95-0.90_1e-06-2e-06/policy/action_discretizer/LA3_MC3_CER100.0-decay=1e-05_KLA1e-06-growth=1e-07_TD0.50-0.33_1e-06-2e-06_params=full_vae_optimization/step1010000/eval_elbo11.174\"\n",
    "    '../../saves/LunarLander-v2/models/vae_LS20_MC1_ER10.0-decay=1e-05-min=0_KLA0.0-growth=5e-05_TD0.67-0.50_1e-06-2e-06_seed=20210510_PER-priority_exponent=0.99-WIS_exponent=0.4-WIS_growth_rate=7.5e-05loss_based_priorities_params=full_vae_optimization-relaxed_state_encoding-latent_policy/base',\n",
    "    step=720000,\n",
    "    discrete_action=True\n",
    ")\n",
    "print(\"VAE MDP loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "vae_mdp.eval_policy(eval_env=py_env, labeling_function=labeling_function, num_eval_episodes=20, render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimeStep(step_type=TensorSpec(shape=(), dtype=tf.int32, name='step_type'), reward=TensorSpec(shape=(), dtype=tf.float32, name='reward'), discount=BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)), observation=BoundedTensorSpec(shape=(8,), dtype=tf.float32, name='observation', minimum=array(-3.4028235e+38, dtype=float32), maximum=array(3.4028235e+38, dtype=float32)))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "py_env = suite_gym.load('LunarLanderNoRewardShaping-v2')\n",
    "py_env.reset()\n",
    "tf_env = tf_py_environment.TFPyEnvironment(py_env)\n",
    "tf_env.time_step_spec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=55.57008>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward_metric = tf_metrics.AverageReturnMetric()\n",
    "sac_policy_dir = '../saves/LunarLander-v2/dqn_policy'\n",
    "saved_policy = SavedTFPolicy(sac_policy_dir)\n",
    "dynamic_episode_driver.DynamicEpisodeDriver(tf_env, saved_policy, num_episodes=5,\n",
    "                                            observers=[\n",
    "                                                lambda _: py_env.render(mode='human'),\n",
    "                                                reward_metric\n",
    "                                            ]).run()\n",
    "reward_metric.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimeStep(step_type=TensorSpec(shape=(), dtype=tf.int32, name='step_type'), reward=TensorSpec(shape=(), dtype=tf.float32, name='reward'), discount=BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)), observation=BoundedTensorSpec(shape=(9,), dtype=tf.float32, name='observation', minimum=array(-3.4028235e+38, dtype=float32), maximum=array(3.4028235e+38, dtype=float32)))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "py_env = suite_gym.load('LunarLanderRewardShapingAugmented-v2')\n",
    "py_env.reset()\n",
    "tf_env = tf_py_environment.TFPyEnvironment(py_env)\n",
    "tf_env.time_step_spec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.policies.tf_policy import TFPolicy\n",
    "from tf_agents.typing import types\n",
    "from tf_agents import specs\n",
    "\n",
    "class MyPolicy(TFPolicy):\n",
    "    \n",
    "    def __init__(self, tf_policy: TFPolicy):\n",
    "        \n",
    "        observation_spec = specs.BoundedTensorSpec(\n",
    "        shape=tf.TensorShape(saved_policy.time_step_spec.observation.shape[:-1] +\n",
    "                             (saved_policy.time_step_spec.observation.shape[-1] + 1)),\n",
    "        dtype=tf_policy.time_step_spec.observation.dtype,\n",
    "        name=tf_policy.time_step_spec.observation.name,\n",
    "        minimum=tf_policy.time_step_spec.observation.minimum,\n",
    "        maximum=tf_policy.time_step_spec.observation.maximum)\n",
    "\n",
    "        super().__init__(tf_policy.time_step_spec._replace(observation=observation_spec), tf_policy.action_spec)\n",
    "        self.wrapped_tf_policy = tf_policy\n",
    "        \n",
    "    def _distribution(self, time_step: ts.TimeStep, policy_state: types.NestedTensorSpec) -> policy_step.PolicyStep:\n",
    "        _time_step = time_step._replace(observation=time_step.observation[..., :-1])\n",
    "        return self.wrapped_tf_policy._distribution(_time_step, policy_state)\n",
    "\n",
    "    def _get_initial_state(self, batch_size: int) -> types.NestedTensor:\n",
    "        return self.wrapped_tf_policy._get_initial_state(batch_size)\n",
    "\n",
    "    def _variables(self):\n",
    "        return self.wrapped_tf_policy._variables()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=260.72534>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward_metric = tf_metrics.AverageReturnMetric()\n",
    "policy_dir = '../saves/LunarLander-v2/dqn_policy'\n",
    "saved_policy = SavedTFPolicy(policy_dir)\n",
    "augmented_policy = MyPolicy(saved_policy)\n",
    "dynamic_episode_driver.DynamicEpisodeDriver(tf_env, augmented_policy, num_episodes=5,\n",
    "                                            observers=[\n",
    "                                                lambda _: py_env.render(mode='human'),\n",
    "                                                reward_metric\n",
    "                                            ]).run()\n",
    "reward_metric.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimeStep(step_type=TensorSpec(shape=(), dtype=tf.int32, name='step_type'), reward=TensorSpec(shape=(), dtype=tf.float32, name='reward'), discount=BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)), observation=BoundedTensorSpec(shape=(9,), dtype=tf.float32, name='observation', minimum=array(-3.4028235e+38, dtype=float32), maximum=array(3.4028235e+38, dtype=float32)))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "augmented_policy = MyPolicy(saved_policy)\n",
    "augmented_policy.time_step_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as restored_function_body, restored_function_body, restored_function_body, restored_function_body, restored_function_body while saving (showing 5 of 30). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as restored_function_body, restored_function_body, restored_function_body, restored_function_body, restored_function_body while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../saves/LunarLander-v2/dqn_policy_reward_shaping_augmented/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../saves/LunarLander-v2/dqn_policy_reward_shaping_augmented/assets\n"
     ]
    }
   ],
   "source": [
    "from tf_agents.policies import policy_saver\n",
    "\n",
    "saver = policy_saver.PolicySaver(augmented_policy)\n",
    "saver.save(os.path.join('..', 'saves', 'LunarLander-v2', 'dqn_policy_reward_shaping_augmented'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=237.3409>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward_metric = tf_metrics.AverageReturnMetric()\n",
    "policy_dir = '../saves/LunarLander-v2/dqn_policy_reward_shaping_augmented'\n",
    "saved_policy = SavedTFPolicy(policy_dir)\n",
    "dynamic_episode_driver.DynamicEpisodeDriver(tf_env, saved_policy, num_episodes=5,\n",
    "                                            observers=[\n",
    "                                                lambda _: py_env.render(mode='human'),\n",
    "                                                reward_metric\n",
    "                                            ]).run()\n",
    "reward_metric.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
