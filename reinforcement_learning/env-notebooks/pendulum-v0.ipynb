{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "from typing import Tuple\n",
    "from tensorflow.python.keras.utils.generic_utils import Progbar\n",
    "from tf_agents.environments import suite_gym, parallel_py_environment\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer, episodic_replay_buffer\n",
    "from tf_agents.trajectories.trajectory import Trajectory\n",
    "from tf_agents.networks import actor_distribution_network\n",
    "from tf_agents.drivers import dynamic_episode_driver, dynamic_step_driver\n",
    "from tf_agents.policies import random_tf_policy\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tf_agents.utils import common\n",
    "from reinforcement_learning.sac_training import NumberOfSafetyViolations\n",
    "from tf_agents.trajectories import time_step as ts, policy_step, trajectory\n",
    "from reinforcement_learning import sac_training\n",
    "from reinforcement_learning import labeling_functions\n",
    "labeling_function = labeling_functions['Pendulum-v0']\n",
    "from util.io.dataset_generator import map_rl_trajectory_to_vae_input\n",
    "from util.io.dataset_generator import ErgodicMDPTransitionGenerator"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Bipedal walker v2 environment\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "TimeStep(step_type=TensorSpec(shape=(), dtype=tf.int32, name='step_type'), reward=TensorSpec(shape=(), dtype=tf.float32, name='reward'), discount=BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)), observation=BoundedTensorSpec(shape=(3,), dtype=tf.float32, name='observation', minimum=array([-1., -1., -8.], dtype=float32), maximum=array([1., 1., 8.], dtype=float32)))"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "py_env = suite_gym.load('Pendulum-v0')\n",
    "py_env.reset()\n",
    "tf_env = tf_py_environment.TFPyEnvironment(py_env)\n",
    "tf_env.time_step_spec()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def display_safe_labeling(trajectory):\n",
    "    label = labeling_functions['Pendulum-v0'](trajectory.observation)\n",
    "    if tf.reduce_any(label):\n",
    "        print(label)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Perform a few steps in this environment\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "replay_buffer_capacity = 1280\n",
    "# specs\n",
    "action_spec = tf_env.action_spec()\n",
    "policy_step_spec = policy_step.PolicyStep(\n",
    "    action=action_spec,\n",
    "    state=(),\n",
    "    info=())\n",
    "trajectory_spec = trajectory.from_transition(tf_env.time_step_spec(),\n",
    "                                             policy_step_spec,\n",
    "                                             tf_env.time_step_spec())\n",
    "\n",
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=trajectory_spec,\n",
    "    batch_size=tf_env.batch_size,\n",
    "    max_length=replay_buffer_capacity)\n",
    "\n",
    "dataset_generator = lambda: replay_buffer.as_dataset(\n",
    "    num_parallel_calls=tf.data.experimental.AUTOTUNE,\n",
    "    num_steps=2\n",
    ").map(\n",
    "    map_func=lambda trajectory, _: map_rl_trajectory_to_vae_input(trajectory, labeling_function),\n",
    "    num_parallel_calls=tf.data.experimental.AUTOTUNE,\n",
    "    #  deterministic=False  # TF version >= 2.2.0\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "(TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 3), dtype=float32, numpy=array([[-0.80730134, -0.5901394 ,  0.9964641 ]], dtype=float32)>),\n ())"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_env = tf_py_environment.TFPyEnvironment(py_env)\n",
    "\n",
    "sac_policy_dir = '../saves/Pendulum-v0/policy/permissive_variance_policy-multiplier=15.0'\n",
    "policy = tf.compat.v2.saved_model.load(sac_policy_dir)\n",
    "dynamic_episode_driver.DynamicEpisodeDriver(tf_env, policy, num_episodes=5,\n",
    "                                            observers=[\n",
    "                                                # display_safe_labeling,\n",
    "                                                lambda _: py_env.render(mode='human'),\n",
    "                                                replay_buffer.add_batch\n",
    "                                            ]).run()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "state tf.Tensor([ 0.99578947 -0.09166978 -0.30986083], shape=(3,), dtype=float32)\n",
      "\n",
      "labels tf.Tensor(\n",
      "[[1. 1. 1.]\n",
      " [1. 1. 1.]], shape=(2, 3), dtype=float32)\n",
      "\n",
      "label tf.Tensor([1. 1. 1.], shape=(3,), dtype=float32)\n",
      "\n",
      "action tf.Tensor([1.4541345], shape=(1,), dtype=float32)\n",
      "\n",
      "reward tf.Tensor(-0.020142874, shape=(), dtype=float32)\n",
      "\n",
      "next_state tf.Tensor([ 0.99502176 -0.0996576  -0.16049299], shape=(3,), dtype=float32)\n",
      "\n",
      "next_label tf.Tensor([1. 1. 1.], shape=(3,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "dataset = replay_buffer.as_dataset(\n",
    "    num_parallel_calls=tf.data.experimental.AUTOTUNE,\n",
    "    num_steps=2\n",
    ")\n",
    "iterator = iter(dataset)\n",
    "trajectory, _ = next(iterator)\n",
    "\n",
    "state = trajectory.observation[0, ...]\n",
    "labels = tf.cast(labeling_function(trajectory.observation), tf.float32)\n",
    "if tf.rank(labels) == 1:\n",
    "    labels = tf.expand_dims(labels, axis=-1)\n",
    "label = labels[0, ...]\n",
    "action = trajectory.action[0, ...]\n",
    "reward = trajectory.reward[0, ...]\n",
    "if tf.rank(reward) == 1:\n",
    "    reward = tf.expand_dims(reward, axis=-1)\n",
    "next_state = trajectory.observation[1, ...]\n",
    "next_label = labels[1, ...]\n",
    "\n",
    "print(\"\\nstate\", state)\n",
    "print('\\nlabels', labels)\n",
    "print('\\nlabel', label)\n",
    "print('\\naction', action)\n",
    "print('\\nreward', reward)\n",
    "print('\\nnext_state', next_state)\n",
    "print('\\nnext_label', next_label)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "(<tf.Tensor: shape=(8, 3), dtype=float32, numpy=\n array([[ 0.9996931 , -0.02477366,  0.13431005],\n        [ 0.9979576 , -0.06387956,  0.29133275],\n        [ 0.99604154, -0.08888876,  0.20481324],\n        [ 0.99731207, -0.07327132,  0.02363477],\n        [ 0.9993869 , -0.03501188, -0.33112368],\n        [ 0.998584  , -0.05319791,  0.25009814],\n        [ 0.9971614 , -0.07529349, -0.15001646],\n        [ 0.9996998 , -0.02450128,  0.10047226]], dtype=float32)>,\n <tf.Tensor: shape=(8, 4), dtype=float32, numpy=\n array([[1., 1., 1., 0.],\n        [1., 1., 1., 0.],\n        [1., 1., 1., 0.],\n        [1., 1., 1., 0.],\n        [1., 1., 1., 0.],\n        [1., 1., 1., 0.],\n        [1., 1., 1., 0.],\n        [1., 1., 1., 0.]], dtype=float32)>,\n <tf.Tensor: shape=(8, 1), dtype=float32, numpy=\n array([[-0.64900994],\n        [-0.08525274],\n        [ 1.4327046 ],\n        [ 1.3229486 ],\n        [-0.02252978],\n        [-0.3857172 ],\n        [ 1.4918438 ],\n        [-0.69871855]], dtype=float32)>,\n <tf.Tensor: shape=(8, 1), dtype=float32, numpy=\n array([[-0.00283899],\n        [-0.01258091],\n        [-0.0141696 ],\n        [-0.00718437],\n        [-0.01219113],\n        [-0.00923638],\n        [-0.01015595],\n        [-0.00209811]], dtype=float32)>,\n <tf.Tensor: shape=(8, 3), dtype=float32, numpy=\n array([[ 0.99971545, -0.02385502,  0.01837831],\n        [ 0.9986279 , -0.05236736,  0.23063518],\n        [ 0.9974554 , -0.07129308,  0.35305235],\n        [ 0.99788946, -0.06493514,  0.16712357],\n        [ 0.9985928 , -0.05303225, -0.36076203],\n        [ 0.99896026, -0.04559012,  0.15234213],\n        [ 0.9972261 , -0.07443141,  0.01729   ],\n        [ 0.99967134, -0.02563649, -0.02271147]], dtype=float32)>,\n <tf.Tensor: shape=(8, 4), dtype=float32, numpy=\n array([[1., 1., 1., 0.],\n        [1., 1., 1., 0.],\n        [1., 1., 1., 0.],\n        [1., 1., 1., 0.],\n        [1., 1., 1., 0.],\n        [1., 1., 1., 0.],\n        [1., 1., 1., 0.],\n        [1., 1., 1., 0.]], dtype=float32)>)"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator = ErgodicMDPTransitionGenerator(labeling_function, replay_buffer)\n",
    "dataset = replay_buffer.as_dataset(\n",
    "    num_parallel_calls=4,\n",
    "    num_steps=2\n",
    ").map(\n",
    "    map_func=generator,\n",
    "    num_parallel_calls=4,\n",
    "    #  deterministic=False  # TF version >= 2.2.0\n",
    ").batch(batch_size=8, drop_remainder=True)\n",
    "iterator = iter(dataset)\n",
    "next(iterator)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAE MDP loaded\n",
      "avg rewards tf.Tensor(-950.75665, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import variational_action_discretizer\n",
    "\n",
    "vae_mdp = variational_action_discretizer.load(\n",
    "    \"../../saves/Pendulum-v0/models/vae_LS13_MC1_ER20.0-decay=7.5e-05-min=-10_KLA0.0-growth=1e-06_TD0.70-0.50_1e-06-2e-06/policy/action_discretizer/LA5_MC1_ER20.0-decay=7.5e-05-min=-10_KLA0.0-growth=1e-06_TD0.25-0.17_1e-06-2e-06_params=full_vae_optimization/step70000/eval_elbo-4.049\"\n",
    "    # \"../../saves/Pendulum-v0/models/vae_LS13_MC1_ER20.0-decay=2e-05-min=-10_KLA0.0-growth=1e-06_TD0.70-0.50_1e-06-2e-06/policy/action_discretizer/LA5_MC1_ER20.0-decay=2e-05-min=-10_KLA0.0-growth=1e-06_TD0.25-0.17_1e-06-2e-06_params=full_vae_optimization/step270000/eval_elbo-1.704\"\n",
    "    # \"../../saves/Pendulum-v0/models/vae_LS13_MC1_ER20.0-decay=7.5e-05-min=-10_KLA0.0-growth=1e-06_TD0.70-0.50_1e-06-2e-06/permissive_variance_policy-multiplier=10.0/action_discretizer/LA5_MC1_ER20.0-decay=7.5e-05-min=-10_KLA0.0-growth=1e-06_TD0.25-0.17_1e-06-2e-06_params=full_vae_optimization/step70000/eval_elbo-7.265\"\n",
    ")\n",
    "print(\"VAE MDP loaded\")\n",
    "\n",
    "discrete_tf_env = vae_mdp.wrap_tf_environment(\n",
    "    tf_env=tf_env,\n",
    "    labeling_function=labeling_functions['Pendulum-v0'],\n",
    "    deterministic_embedding_functions=False\n",
    ")\n",
    "discrete_tf_env.reset()\n",
    "\n",
    "reward_metric = tf_metrics.AverageReturnMetric()\n",
    "\n",
    "policy = vae_mdp.get_abstract_policy()\n",
    "\n",
    "dynamic_episode_driver.DynamicEpisodeDriver(\n",
    "    discrete_tf_env,\n",
    "    policy,\n",
    "    num_episodes=20,\n",
    "    observers=[\n",
    "        lambda _: py_env.render(mode='human'),\n",
    "        reward_metric\n",
    "    ]\n",
    ").run()\n",
    "\n",
    "print(\"avg rewards\", reward_metric.result())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}