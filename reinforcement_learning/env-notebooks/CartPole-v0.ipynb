{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import os\n",
    "import tf_agents.policies\n",
    "import tf_agents.specs\n",
    "from tf_agents.environments import suite_gym, parallel_py_environment\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.policies import policy_saver\n",
    "from tf_agents.policies.tf_py_policy import TFPyPolicy\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer, episodic_replay_buffer\n",
    "from tf_agents.drivers import dynamic_episode_driver, dynamic_step_driver\n",
    "import tensorflow as tf\n",
    "tf.config.set_visible_devices([], 'GPU')  #  allows testing during training\n",
    "from tf_agents.trajectories import time_step as ts, policy_step, trajectory\n",
    "from reinforcement_learning import labeling_functions\n",
    "labeling_function = labeling_functions['CartPole-v0']\n",
    "from util.io.dataset_generator import map_rl_trajectory_to_vae_input\n",
    "from util.io.dataset_generator import ErgodicMDPTransitionGenerator"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% CartPole-v0 environment\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "TimeStep(step_type=TensorSpec(shape=(), dtype=tf.int32, name='step_type'), reward=TensorSpec(shape=(), dtype=tf.float32, name='reward'), discount=BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)), observation=BoundedTensorSpec(shape=(4,), dtype=tf.float32, name='observation', minimum=array([-4.8000002e+00, -3.4028235e+38, -4.1887903e-01, -3.4028235e+38],\n      dtype=float32), maximum=array([4.8000002e+00, 3.4028235e+38, 4.1887903e-01, 3.4028235e+38],\n      dtype=float32)))"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "py_env = suite_gym.load('CartPole-v0')\n",
    "py_env.reset()\n",
    "tf_env = tf_py_environment.TFPyEnvironment(py_env)\n",
    "tf_env.time_step_spec()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def display_safe_labeling(trajectory):\n",
    "    label = labeling_functions['CartPole-v0'](trajectory.observation)\n",
    "    if tf.reduce_any(label):\n",
    "        print(label)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Perform a few steps in this environment\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "tf_env = tf_py_environment.TFPyEnvironment(py_env)\n",
    "\n",
    "policy_dir = '../saves/CartPole-v0/policy/permissive_policy_temperature=1e+06'\n",
    "# policy = tf_agents.policies.py_tf_eager_policy.SavedModelPyTFEagerPolicy(policy_dir, load_specs_from_pbtxt=True)\n",
    "# policy = TFPyPolicy(policy=policy)\n",
    "spec_path = os.path.join(policy_dir, policy_saver.COLLECT_POLICY_SPEC)\n",
    "policy_specs = tf_agents.specs.tensor_spec.from_pbtxt_file(spec_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "replay_buffer_capacity = 1280\n",
    "# specs\n",
    "collect_data_spec = policy_specs['collect_data_spec']\n",
    "policy_state_spec = policy_specs['policy_state_spec']\n",
    "policy_step_spec =  policy_step.PolicyStep(\n",
    "    action=collect_data_spec.action,\n",
    "    state=policy_state_spec,\n",
    "    info=collect_data_spec.policy_info)\n",
    "trajectory_spec = trajectory.from_transition(tf_env.time_step_spec(),\n",
    "                                             policy_step_spec,\n",
    "                                             tf_env.time_step_spec())\n",
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=trajectory_spec,\n",
    "    batch_size=tf_env.batch_size,\n",
    "    max_length=replay_buffer_capacity)\n",
    "\n",
    "dataset_generator = lambda: replay_buffer.as_dataset(\n",
    "    num_parallel_calls=tf.data.experimental.AUTOTUNE,\n",
    "    num_steps=2\n",
    ").map(\n",
    "    map_func=lambda trajectory, _: map_rl_trajectory_to_vae_input(trajectory, labeling_function),\n",
    "    num_parallel_calls=tf.data.experimental.AUTOTUNE,\n",
    "    #  deterministic=False  # TF version >= 2.2.0\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(), dtype=float32, numpy=200.0>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy = tf.compat.v2.saved_model.load(policy_dir)\n",
    "reward_metric = tf_metrics.AverageReturnMetric()\n",
    "dynamic_episode_driver.DynamicEpisodeDriver(tf_env, policy, num_episodes=5,\n",
    "                                            observers=[\n",
    "                                                # display_safe_labeling,\n",
    "                                                lambda _: py_env.render(mode='human'),\n",
    "                                                replay_buffer.add_batch,\n",
    "                                                reward_metric\n",
    "                                            ]).run()\n",
    "reward_metric.result()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset = replay_buffer.as_dataset(\n",
    "    num_parallel_calls=tf.data.experimental.AUTOTUNE,\n",
    "    num_steps=2\n",
    ")\n",
    "iterator = iter(dataset)\n",
    "trajectory, _ = next(iterator)\n",
    "\n",
    "state = trajectory.observation[0, ...]\n",
    "labels = tf.cast(labeling_function(trajectory.observation), tf.float32)\n",
    "if tf.rank(labels) == 1:\n",
    "    labels = tf.expand_dims(labels, axis=-1)\n",
    "label = labels[0, ...]\n",
    "action = trajectory.action[0, ...]\n",
    "reward = trajectory.reward[0, ...]\n",
    "if tf.rank(reward) == 1:\n",
    "    reward = tf.expand_dims(reward, axis=-1)\n",
    "next_state = trajectory.observation[1, ...]\n",
    "next_label = labels[1, ...]\n",
    "\n",
    "print(\"\\nstate\", state)\n",
    "print('\\nlabels', labels)\n",
    "print('\\nlabel', label)\n",
    "print('\\naction', action)\n",
    "print('\\nreward', reward)\n",
    "print('\\nnext_state', next_state)\n",
    "print('\\nnext_label', next_label)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "generator = ErgodicMDPTransitionGenerator(labeling_function, replay_buffer)\n",
    "dataset = replay_buffer.as_dataset(\n",
    "    num_parallel_calls=4,\n",
    "    num_steps=2\n",
    ").map(\n",
    "    map_func=generator,\n",
    "    num_parallel_calls=4,\n",
    "    #  deterministic=False  # TF version >= 2.2.0\n",
    ").batch(batch_size=8, drop_remainder=True)\n",
    "iterator = iter(dataset)\n",
    "next(iterator)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Importing a function (__inference_encoder_latent_distribution_logits_layer_call_and_return_conditional_losses_769000) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:tensorflow:Importing a function (__inference_encoder_latent_distribution_logits_layer_call_and_return_conditional_losses_769000) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:tensorflow:Importing a function (__inference_encoder_latent_distribution_logits_layer_call_and_return_conditional_losses_769000) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:tensorflow:Importing a function (__inference_encoder_layer_call_and_return_conditional_losses_773629) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:tensorflow:Importing a function (__inference_encoder_layer_call_and_return_conditional_losses_773629) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:tensorflow:Importing a function (__inference_encoder_layer_call_and_return_conditional_losses_773629) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:tensorflow:Importing a function (__inference_entropy_regularizer_773561) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:tensorflow:Importing a function (__inference_entropy_regularizer_773561) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:tensorflow:Importing a function (__inference_entropy_regularizer_773561) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:tensorflow:Importing a function (__inference_encoder_layer_call_and_return_conditional_losses_773697) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:tensorflow:Importing a function (__inference_encoder_layer_call_and_return_conditional_losses_773697) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:tensorflow:Importing a function (__inference_encoder_layer_call_and_return_conditional_losses_773697) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:tensorflow:Importing a function (__inference__wrapped_model_768779) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:tensorflow:Importing a function (__inference__wrapped_model_768779) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:tensorflow:Importing a function (__inference__wrapped_model_768779) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:tensorflow:Importing a function (__inference_encoder_latent_distribution_logits_layer_call_and_return_conditional_losses_774791) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:tensorflow:Importing a function (__inference_encoder_latent_distribution_logits_layer_call_and_return_conditional_losses_774791) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:tensorflow:Importing a function (__inference_encoder_latent_distribution_logits_layer_call_and_return_conditional_losses_774791) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:tensorflow:Importing a function (__inference_entropy_regularizer_768034) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:tensorflow:Importing a function (__inference_entropy_regularizer_768034) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:tensorflow:Importing a function (__inference_entropy_regularizer_768034) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:tensorflow:Importing a function (__inference___call___773334) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:tensorflow:Importing a function (__inference___call___773334) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:tensorflow:Importing a function (__inference___call___773334) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:tensorflow:Importing a function (__inference___call___773334) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:tensorflow:Importing a function (__inference___call___773334) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:tensorflow:Importing a function (__inference___call___773334) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:tensorflow:Importing a function (__inference___call___768554) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:tensorflow:Importing a function (__inference___call___768554) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:tensorflow:Importing a function (__inference___call___768554) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:tensorflow:Importing a function (__inference___call___768554) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:tensorflow:Importing a function (__inference___call___768554) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "WARNING:tensorflow:Importing a function (__inference___call___768554) with ops with custom gradients. Will likely fail if a gradient is requested.\n",
      "VAE MDP loaded\n",
      "avg rewards tf.Tensor(200.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import variational_mdp\n",
    "\n",
    "vae_mdp = variational_mdp.load(\n",
    "    # \"../../saves/Pendulum-v0/models/vae_LS12_MC1_ER20.0-decay=7.5e-05-min=-10_KLA0.0-growth=1e-06_TD0.70-0.50_1e-06-2e-06/policy/action_discretizer/LA3_MC1_ER20.0-decay=7.5e-05-min=-10_KLA0.0-growth=1e-06_TD0.50-0.33_1e-06-2e-06_params=full_vae_optimization/step140000/eval_elbo-3.784\"\n",
    "    # \"../../saves/Pendulum-v0/models/vae_LS13_MC1_ER20.0-decay=2e-05-min=-10_KLA0.0-growth=1e-06_TD0.70-0.50_1e-06-2e-06/policy/action_discretizer/LA5_MC1_ER20.0-decay=2e-05-min=-10_KLA0.0-growth=1e-06_TD0.25-0.17_1e-06-2e-06_params=full_vae_optimization/step270000/eval_elbo-1.704\"\n",
    "    # \"../../saves/Pendulum-v0/models/vae_LS13_MC1_ER20.0-decay=7.5e-05-min=-10_KLA0.0-growth=1e-06_TD0.70-0.50_1e-06-2e-06/permissive_variance_policy-multiplier=10.0/action_discretizer/LA5_MC1_ER20.0-decay=7.5e-05-min=-10_KLA0.0-growth=1e-06_TD0.25-0.17_1e-06-2e-06_params=full_vae_optimization/step70000/eval_elbo-7.265\"\n",
    "    # \"../../saves/Pendulum-v0/models/vae_LS12_MC3_CER100.0-decay=1e-05_KLA1e-06-growth=1e-07_TD0.95-0.90_1e-06-2e-06/policy/action_discretizer/LA3_MC3_CER100.0-decay=1e-05_KLA1e-06-growth=1e-07_TD0.50-0.33_1e-06-2e-06_params=full_vae_optimization/step1010000/eval_elbo11.174\"\n",
    "    '../../saves/CartPole-v0/models/vae_LS12_MC1_ER10.0-decay=0.00025-min=0_KLA0.0-growth=5e-06_TD0.67-0.50_1e-06-2e-06_seed=60421_params=relaxed_state_encoding-latent_policy/base',\n",
    "    # '../../saves/CartPole-v0/models/vae_LS12_MC1_ER10.0-decay=0.00025-min=0_KLA0.0-growth=5e-06_TD0.67-0.50_1e-06-2e-06_seed=20421_params=relaxed_state_encoding-latent_policy/base',\n",
    "    step=100000, discrete_action=True\n",
    ")\n",
    "print(\"VAE MDP loaded\")\n",
    "\n",
    "discrete_tf_env = vae_mdp.wrap_tf_environment(\n",
    "    tf_env=tf_env,\n",
    "    labeling_function=labeling_function,\n",
    "    deterministic_embedding_functions=True\n",
    ")\n",
    "discrete_tf_env.reset()\n",
    "\n",
    "reward_metric = tf_metrics.AverageReturnMetric()\n",
    "\n",
    "policy = vae_mdp.get_latent_policy()\n",
    "\n",
    "dynamic_episode_driver.DynamicEpisodeDriver(\n",
    "    discrete_tf_env,\n",
    "    policy,\n",
    "    num_episodes=10,\n",
    "    observers=[\n",
    "        lambda _: py_env.render(mode='human'),\n",
    "        reward_metric\n",
    "    ]\n",
    ").run()\n",
    "\n",
    "print(\"avg rewards\", reward_metric.result())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}